{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prepare_foodbase.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO43UOZfN+IzZNtKD1PDoY/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Connect to Google Drive"],"metadata":{"id":"0gJ_ir5xVtpT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHhlI83gVoqA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646577545552,"user_tz":-60,"elapsed":20524,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"9251bce3-564d-43d0-c4fb-e4537e8b2f55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/BachelorThesis/datasets/foodBase\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/BachelorThesis/datasets/foodBase"]},{"cell_type":"markdown","source":["# Installations"],"metadata":{"id":"MAod3NWdVz0s"}},{"cell_type":"markdown","source":["The Github Repo to the paper \"\" by ...(2021) provides the FoodBase dataset annotated with 5 different schemes: \n","- food-classification\n","- foodon\n","- hansard-closest\n","- hansard-parent\n","- snomedct\n","(See the paper for information on the individual tagging schemes)\n","\n","Let's load this data:"],"metadata":{"id":"eBjj3eV7WCYJ"}},{"cell_type":"code","source":["# !git clone https://github.com/ds4food/FoodNer.git # download the prepared foodcorpus data"],"metadata":{"id":"vpVDOKgWVzPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The downloaded data includes a train and test file for each of the 5 different schemes. Additionally, the single recipes in those files are all concatenated, so that the beginning and end of a recipe can no longer be clearly determined.\n","\n","\n","In the following sections, the data is preprocessed, so that all information from all those 10 files is gathered in one single json file. Furthermore, each line of this file will represent one recipe and its different tagging results, so that it is compatible with the run_ner.py script from huggingface, that is used for finetuning on NER.\n","\n","**Example of the desired format**:\n","\n","{\"id\": recipe1, \"category\": \"Drinks\", \"words:\" [\"Add\", \"apples\"], \"ner-food-classification\": [\"O\", \"FOOD\"], \"ner-foodon\": ...}\n","\n","{\"id\": recipe2, \"category\": \"Breakfast and Lunch\", \"words:\" [\"Oats\", \"stink\"], \"ner-food-classification\": [\"FOOD\", \"O\"], \"ner-foodon\": ...}\n","\n","...\n"],"metadata":{"id":"47MZktzEXQgh"}},{"cell_type":"markdown","source":["# Data Processing"],"metadata":{"id":"RXii5S7FXPcB"}},{"cell_type":"code","source":["# Method for extracting the corresponding named entity tags from the processed foodbase data; this method will take a tokenized recipe and look in whole_data where to find it. Then it returns the corresponding tags\n","\n","# params: whole_data that contains the recipe (=list of tokens), tags of whole_data (= list of the corresponding tags for the whole_data tokens), tokenized_recipe to find (= list of tokens)\n","# Example input:\n","# whole_data =          [\"hi\", \"my\", \"name\", \"is\", \"kevin\", \"and\", \"i\", \"like\", \"it\", \"!\"]\n","# corresponding_tags =  ['0,  '0',    '0',   '0', 'PERSON',  '0',  '0',  '0',   '0',  '0'] \n","# tokenized_recipe =    [\"name\", \"is\", \"kevin\"]\n","# Output:               ['0',    '0', 'PERSON']\n","def extract_tags(whole_data, corresponding_tags, tokenized_recipe):\n","  recipe_len = len(tokenized_recipe)\n","  correct_count = 0 # counting how many tokens matched already; if all tokens of tokenized_recipe are matched, then the recipe was found in the whole data\n","  for i in range(len(whole_data)): # iterate over every token of the entire data/ words\n","    if whole_data[i] == tokenized_recipe[correct_count]:\n","      correct_count += 1\n","      if correct_count == recipe_len: # the tokenized recipe was found in the whole_data list --> return corresponding tags\n","        start = i - recipe_len + 1 # startindex is inclusive\n","        end = i + 1 # endindex is not inclusive\n","        return corresponding_tags[start:end]\n","    else:\n","      correct_count = 0\n","  print(\"No corresponding tags found for: \", tokenized_recipe)  "],"metadata":{"id":"VFCtA1BfXOWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read in the 10 train and test files: "],"metadata":{"id":"BYArNMq2a7xm"}},{"cell_type":"code","source":["# read in the preprocessed data and put it in dict\n","corpora = {\n","         \"food-classification\": {},\n","         \"foodon\": {},\n","         \"hansard-closest\": {},\n","         \"hansard-parent\": {},\n","         \"snomedct\": {}\n","        }\n","\n","for task in corpora:\n","  with open(f\"train-{task}.txt\", \"r\") as train_file, open(f\"test-{task}.txt\", \"r\") as test_file:\n","    next(train_file) # skip first header-line\n","    next(test_file) # skip first header-line\n","    tokens = []\n","    ner = []\n","    for line in train_file.readlines():\n","      token_ner_split = line.rstrip().split('\\t')\n","      tokens.append(token_ner_split[0])\n","      ner.append(token_ner_split[1])\n","    for line in test_file.readlines():\n","      token_ner_split = line.rstrip().split('\\t')\n","      tokens.append(token_ner_split[0]) # append tokens of test_file to the tokens of train_file\n","      ner.append(token_ner_split[1]) # append the tags of test_file to the tags of train_file\n","    corpora[task][\"tokens\"] = tokens\n","    corpora[task][\"ner\"] = ner\n","\n","# corpora dict now has for each of the 5 tasks two lists of same length: list with tokens and list with corresponding named-entity tag\n","# here you can see that for a given text, 5 different tag-sets are provided\n","print(\"text\")\n","print(corpora[\"foodon\"][\"tokens\"][:10])\n","print(\"\\ntags\")\n","for key in corpora:\n","  print(corpora[key][\"ner\"][:10])"],"metadata":{"id":"vEWu0D_ba6sj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646577562068,"user_tz":-60,"elapsed":4340,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"0e11e99a-d0ca-4371-96af-53a94951a162"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["text\n","['Mix', 'the', 'cream', 'cheese', ',', 'beef', ',', 'olives', ',', 'onion']\n","\n","tags\n","['O', 'O', 'B-FOOD', 'I-FOOD', 'O', 'B-FOOD', 'O', 'B-FOOD', 'O', 'B-FOOD']\n","['O', 'O', 'B-FOODON_03301889', 'I-FOODON_03301889', 'O', 'O', 'O', 'O', 'O', 'B-FOODON_03316347']\n","['O', 'O', 'B-AG.01.e.02', 'I-AG.01.e.02', 'O', 'B-AG.01.d.03', 'O', 'B-AG.01.h.01.e', 'O', 'B-AG.01.h.02.e']\n","['O', 'O', 'B-AG.01.e', 'I-AG.01.e', 'O', 'B-AG.01', 'O', 'B-AG.01.h', 'O', 'B-AG.01.h']\n","['O', 'O', 'B-226849005', 'I-226849005', 'O', 'B-226916002', 'O', 'B-227436000', 'O', 'B-735047000']\n"]}]},{"cell_type":"markdown","source":["Now let's read in the official FoodBase data. For each recipe in it, the recipe is tokenized in the same way as it was done to create the 10 train and test files above. The position of the tokenized recipe is then looked up in the corpora-dict that was created before, to find the corresponding tags for each of the 5 tagging schemes.\n"],"metadata":{"id":"KPy9c5jub7xA"}},{"cell_type":"code","source":["# some tokens are not correctly tokenized to produce the same output as the preprocessed recipes from FoodNer,\n","# they will therefore be manually adapted\n","special_tokenize_cases = {\n","    \"butter/margarine\": [\"butter\", \"/\", \"margarine\"],\n","    \"1/2-inch\": [\"1\", \"/\", \"2-inch\"],\n","    \"3/4-full\": [\"3\", \"/\", \"4-full\"],\n","    \"'em\": [\"'\", \"em\"],\n","    \"1/4-inch-thick\": [\"1\", \"/\", \"4-inch-thick\"],\n","    \"tequila/lime\": [\"tequila\", \"/\", \"lime\"],\n","    \"1/4-inch\": [\"1\", \"/\", \"4-inch\"],\n","    \"oil/juice\": [\"oil\", \"/\", \"juice\"],\n","    \"Fruit/Nut\": [\"Fruit\", \"/\", \"Nut\"],\n","    \"1/4\": [\"1\", \"/\", \"4\"],\n","    \"and/or\": [\"and\", \"/\", \"or\"],\n","    \".Grease\": [\".\", \"Grease\"],\n","    \"1/2x11-inch\": [\"1\", \"/\", \"2x11-inch\"],\n","    \"C.\": [\"C\", \".\"],\n","    \"3/4\": [\"3\", \"/\", \"4\"],\n","    \"2/3\": [\"2\", \"/\", \"3\"],\n","    \"1/2\": [\"1\", \"/\", \"2\"],\n","    \"F.\": [\"F\", \".\"],\n","    \"1/3\": [\"1\", \"/\", \"3\"],\n","    \"1/2-cupful\": [\"1\", \"/\", \"2-cupful\"],\n","    \"mushroom/sausage\": [\"mushroom\", \"/\", \"sausage\"],\n","    \"ok.\": [\"ok\", \".\"],\n","    \"1/8\": [\"1\", \"/\", \"8\"],\n","    \"1/2-inch-thick\": [\"1\", \"/\", \"2-inch-thick\"],\n","    \"1/2-ounce\": [\"1\", \"/\", \"2-ounce\"],\n","}"],"metadata":{"id":"hn-66aY8b80U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the official foodbase data\n","from xml.dom import minidom\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","xmldoc = minidom.parse('original_foodbase/FoodBase_curated.xml')\n","recipes = xmldoc.getElementsByTagName('document')\n","\n","recipe_corpus = [] # the adapted/ resulting recipe corpus --> contains a dict for each recipe. the dict contains id, category, words/ tokens of the recipe and the corresponding tags for all 5 tasks\n","\n","for recipe in recipes: # itearate over each recipe in the dataset (1000 in total)\n","  id = recipe.getElementsByTagName('id')[0].firstChild.nodeValue\n","  category = recipe.getElementsByTagName('infon')[0].firstChild.nodeValue\n","  full_text = recipe.getElementsByTagName('infon')[1].firstChild.nodeValue.strip()\n","  tokenized_text = word_tokenize(full_text)\n","  \n","  for idx, word in enumerate(tokenized_text):\n","    if word in special_tokenize_cases: # check for special tokenized words\n","      tokenized_text[idx:idx+1] = special_tokenize_cases[word]\n","  \n","  if \"confectioners\" in tokenized_text: # the token \"confectioners\" followed by \"'\" should be tokenized into \"confectioners'\" which is done manually here, since word_tokenize splits those up\n","    for idx, word in enumerate(tokenized_text):\n","      if word == \"confectioners\" and tokenized_text[idx + 1] == \"'\":\n","        tokenized_text[idx : idx + 2] = [\"confectioners'\"]\n","  if \"!\" in tokenized_text: # the token \"!\" followed by \")\" should be tokenized into \"!)\" which is done manually here, since word_tokenize splits those up\n","    for idx, word in enumerate(tokenized_text):\n","      try:\n","        if word == \"!\" and tokenized_text[idx + 1] == \")\":\n","          tokenized_text[idx : idx + 2] = [\"!)\"]\n","      except:\n","        pass\n","  recipe_entry = {\n","                  \"id\": id,\n","                  \"category\": category,\n","                  \"words\": tokenized_text, \n","                }\n","  for task in corpora: # iterate over each of the 5 tagging schemes\n","    tags_for_recipe = extract_tags(corpora[task]['tokens'], corpora[task]['ner'], tokenized_text) # extract tags of current tagging scheme for current recipe\n","    recipe_entry[f\"ner-{task}\"] = tags_for_recipe\n","  recipe_corpus.append(recipe_entry)\n","\n","print(\"Number of final recipes: \", len(recipe_corpus))\n","print(\"Example entry: \", recipe_corpus[0])"],"metadata":{"id":"J1mAsCw5cE_Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646578166415,"user_tz":-60,"elapsed":33093,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"bebf5e09-4bc5-4300-cb14-614520c3e4c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Number of final recipes:  1000\n","Example entry:  {'id': '0recipe1006', 'category': 'Appetizers and snacks', 'words': ['Mix', 'the', 'cream', 'cheese', ',', 'beef', ',', 'olives', ',', 'onion', ',', 'and', 'Worcestershire', 'sauce', 'together', 'in', 'a', 'bowl', 'until', 'evenly', 'blended', '.', 'Keeping', 'the', 'mixture', 'in', 'the', 'bowl', ',', 'scrape', 'it', 'into', 'a', 'semi-ball', 'shape', '.', 'Cover', ',', 'and', 'refrigerate', 'until', 'firm', ',', 'at', 'least', '2', 'hours', '.', 'Place', 'a', 'large', 'sheet', 'of', 'waxed', 'paper', 'on', 'a', 'flat', 'surface', '.', 'Sprinkle', 'with', 'walnuts', '.', 'Roll', 'the', 'cheese', 'ball', 'in', 'the', 'walnuts', 'until', 'completely', 'covered', '.', 'Transfer', 'the', 'cheese', 'ball', 'to', 'a', 'serving', 'plate', ',', 'or', 'rewrap', 'with', 'waxed', 'paper', 'and', 'refrigerate', 'until', 'needed', '.'], 'ner-food-classification': ['O', 'O', 'B-FOOD', 'I-FOOD', 'O', 'B-FOOD', 'O', 'B-FOOD', 'O', 'B-FOOD', 'O', 'O', 'B-FOOD', 'I-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner-foodon': ['O', 'O', 'B-FOODON_03301889', 'I-FOODON_03301889', 'O', 'O', 'O', 'O', 'O', 'B-FOODON_03316347', 'O', 'O', 'B-FOODON_03305003', 'I-FOODON_03305003', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NCBITaxon_16718', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NCBITaxon_16718', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner-hansard-closest': ['O', 'O', 'B-AG.01.e.02', 'I-AG.01.e.02', 'O', 'B-AG.01.d.03', 'O', 'B-AG.01.h.01.e', 'O', 'B-AG.01.h.02.e', 'O', 'O', 'B-AG.01.l.04', 'I-AG.01.l.04', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AF.20.c', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AF.20.c', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner-hansard-parent': ['O', 'O', 'B-AG.01.e', 'I-AG.01.e', 'O', 'B-AG.01', 'O', 'B-AG.01.h', 'O', 'B-AG.01.h', 'O', 'O', 'B-AG.01.h', 'I-AG.01.h', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AG.01.h', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AG.01.h', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner-snomedct': ['O', 'O', 'B-226849005', 'I-226849005', 'O', 'B-226916002', 'O', 'B-227436000', 'O', 'B-735047000', 'O', 'O', 'B-443701000124100', 'I-443701000124100', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"]}]},{"cell_type":"markdown","source":["As a final step the data is now shuffled in a way, to enable stratified cross-validation. There are 5 categories a recipe can belong to on each for each of these 5 categories, 200 recipes are in the dataset. For stratified shuffling, the data is split into its 5 categorie-sets. Each of them is randomly shuffled a then rotatory appended again to the dataset. For 10 fold cross-validation, the data can then be loaded and just be fold into 10 parts (based on position). This ensures that each fold has the same amount of recipes for each split."],"metadata":{"id":"OJxpftEwf9yJ"}},{"cell_type":"code","source":["# save shuffled data to json file\n","import json\n","import random\n","import os.path\n","\n","path = 'foodbase_with_tags_for_5_tasks.json'\n","\n","if not os.path.exists(path):\n","  # extract recipes for each category\n","  category_appetizers = [recipe for recipe in recipe_corpus if recipe['category'] == \"Appetizers and snacks\"]\n","  category_breakfast = [recipe for recipe in recipe_corpus if recipe['category'] == \"Breakfast and Lunch\"]\n","  category_desserts = [recipe for recipe in recipe_corpus if recipe['category'] == \"Desserts\"]\n","  category_dinners = [recipe for recipe in recipe_corpus if recipe['category'] == \"Dinners\"]\n","  category_drinks = [recipe for recipe in recipe_corpus if recipe['category'] == \"Drinks\"]\n","\n","  # shuffle each category randomly\n","  random.shuffle(category_appetizers)\n","  random.shuffle(category_breakfast)\n","  random.shuffle(category_desserts)\n","  random.shuffle(category_dinners)\n","  random.shuffle(category_drinks)\n","\n","  stratified_shuffled_recipes = []\n","  # rotatory append the shuffled data\n","  for i in range(len(category_appetizers)):\n","    stratified_shuffled_recipes.append(category_appetizers.pop())\n","    stratified_shuffled_recipes.append(category_breakfast.pop())\n","    stratified_shuffled_recipes.append(category_desserts.pop())\n","    stratified_shuffled_recipes.append(category_dinners.pop())\n","    stratified_shuffled_recipes.append(category_drinks.pop())\n","\n","  # save final data\n","  with open(path, 'w') as file:\n","    for recipe in stratified_shuffled_recipes:\n","      json.dump(recipe, file)\n","      file.write(\"\\n\")\n","else:\n","  print(\"file already exists\")"],"metadata":{"id":"WmJThI1xf8YK"},"execution_count":null,"outputs":[]}]}