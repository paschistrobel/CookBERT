{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"create_domain_vocabs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZJjP+IYrg+M55KeI+gDxj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Connect to Google Drive"],"metadata":{"id":"1Za6vJxijVcb"}},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/BachelorThesis/domain_analysis"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBvxnQCEjTwZ","executionInfo":{"status":"ok","timestamp":1642774744727,"user_tz":-60,"elapsed":21178,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"7e1f496e-3ec2-4a11-8283-aa4af3dcc1d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/BachelorThesis/domain_analysis\n"]}]},{"cell_type":"code","source":["import sys \n","l = [\"Hallo du kek\", \"kalsdjfkoasdjf asodf iasdo f\", \" lasdfsa \"]\n","print(sys.getsizeof(l))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUBpfy5ktRnT","executionInfo":{"status":"ok","timestamp":1645168938814,"user_tz":-60,"elapsed":26,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"473977ac-cb82-4d5f-b265-416c4f552a88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["96\n"]}]},{"cell_type":"markdown","source":["# Create vocabulary for datasets\n","This notebooks creates the vocabulary of the 4 datasets:\n","\n","*   RecipeNLG (instructions)\n","*   Recipe1M+ (instructions)\n","*   Allrecipes.com\n","*   Wikipedia and book corpus\n","\n","The generated vocabulary- json files will later be used to measure the vocabulary overlap between the datasets.\n","\n","The created vocabularies are in the form of a dictionary with\n",">key = token <br>\n",">value = frequency of token>\n","\n","Example:\n","```\n","{\n","  \"token1\": 2021,\n","  \"token2\": 1019,\n","  \"token3\": 988,\n","  ...\n","}\n","```"],"metadata":{"id":"a7BY4cEBbd7p"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbusAL1IaBnb"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","import json\n","import os.path\n","\n","vocab_size = 10000 # number of words the vocabulary of each dataset should consist\n","\n","# define excluded tokens: stopwords and punctuation\n","stops = stopwords.words('english') # stopwords\n","for char in string.punctuation:\n","  stops.append(char) # append punctuation characters to the stopword list\n","print(len(stops), \"excluded stopwords: \", stops)\n","\n","# params --> data: list of text, n: number of considered vocabs, stopwords: list of tokens to be excluded from the vocab\n","def create_vocab_for_most_n_frequent_words(data, n, stopwords = []):\n","  vocab = {}\n","  progress = 0 # progress of vocabulary creation\n","  for chunk in data:\n","    # print current progress\n","    print(f\"Progress: {progress} / {len(data)}\", end = \"\")\n","    progress += 1\n","    # remove punctuation from text\n","    for punct in string.punctuation:\n","      chunk = chunk.replace(punct, ' ')\n","    # lowercase and tokenize text\n","    tokens = word_tokenize(chunk.lower())\n","    # append tokens to vocab and increase the frequency count\n","    for token in tokens:\n","      vocab[token] = vocab.get(token, 0) + 1\n","    print(end=\"\\r\")\n","\n","  # remove stopwords from vocab\n","  for word in stopwords:\n","    if word in vocab:\n","      del vocab[word]\n","      \n","  # sort the vocabulary based on token frequency\n","  vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True) # returns list\n","\n","  # truncate vocabulary to specific size\n","  while len(vocab) > n:\n","    vocab.pop(-1)\n","  return dict(vocab) \n","\n","def save_vocab_to_file(vocab, filename):\n","  if os.path.isfile(filename):\n","    print(f\"Vocab file '{filename}' already exists!\")\n","  else:\n","    with open(filename, 'w') as file:\n","      file.write(json.dumps(vocab))\n","\n","def print_vocab(vocab):\n","  if type(vocab) is not dict:\n","    print(\"'vocab' is not a dictionary\")\n","    return\n","  else:\n","    for key, value in vocab.items():\n","      print(f\"{key}: {value}\")"]},{"cell_type":"markdown","source":["## Create vocab for Recipe1M+ (instructions)"],"metadata":{"id":"BXjxnnWncXHG"}},{"cell_type":"code","source":["data = []\n","with open(\"../datasets/recipe1M/recipe1M_instructions.txt\") as file:\n","    for line in file:\n","      data.append(line.rstrip())\n","\n","recipe1m_vocab = create_vocab_for_most_n_frequent_words(data, vocab_size, stopwords = stops) # this method can take quite a while\n","# print_vocab(recipe1m_vocab)\n","save_vocab_to_file(recipe1m_vocab, \"recipe1M_vocabulary.json\")"],"metadata":{"id":"obUgLFRGbc3v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642776409864,"user_tz":-60,"elapsed":1264502,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"25832302-2cc9-4031-c914-30af5abd6792"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[""]}]},{"cell_type":"markdown","source":["## Create vocab for RecipeNLG (instructions)\n"],"metadata":{"id":"eP_8Hz_Kc7mc"}},{"cell_type":"code","source":["data = []\n","with open(\"../datasets/recipeNLG/recipeNLG_instructions.txt\") as file:\n","    for line in file:\n","      data.append(line.rstrip())\n","\n","recipeNLG_vocab = create_vocab_for_most_n_frequent_words(data, vocab_size, stopwords = stops)\n","# print_vocab(recipeNLG_vocab)\n","save_vocab_to_file(recipeNLG_vocab, \"recipeNLG_vocabulary.json\")"],"metadata":{"id":"XMdKLos-c79l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642778939552,"user_tz":-60,"elapsed":2440673,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"0a9118e9-9d5d-4eb1-91cc-f98d7be24f4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[""]}]},{"cell_type":"markdown","source":["## Create vocab for allrecipes.com"],"metadata":{"id":"YTUxiKjYg7ED"}},{"cell_type":"code","source":["data = []\n","with open(\"../datasets/recipeNLG/allrecipe_instructions.txt\") as file:\n","    for line in file:\n","      data.append(line.rstrip())\n","\n","allrecipes_vocab = create_vocab_for_most_n_frequent_words(data, vocab_size, stopwords = stops)\n","#print_vocab(allrecipes_vocab)\n","save_vocab_to_file(allrecipes_vocab, \"allrecipes_vocabulary.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_a7NuGuSg7Mq","executionInfo":{"status":"ok","timestamp":1642774995736,"user_tz":-60,"elapsed":73633,"user":{"displayName":"Paschi Strobel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjobUJCA_Ck2VfNovLz59RHEuLDra_PAURiUuov=s64","userId":"06759777096404192052"}},"outputId":"9bfddf9d-89f3-4b59-99b1-cf489fb1fff0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[""]}]},{"cell_type":"markdown","source":["## Create vocab for Wikipedia/BookCorpus "],"metadata":{"id":"cHfTP-DEhWeC"}},{"cell_type":"code","source":["import os\n","filenames = os.listdir(\"../datasets/wikipedia_book_corpus/data\")\n","\n","data = []\n","# iterate over files since wikipedia/bookcorpus is made up of several individual files\n","for filename in filenames:\n","  filepath = \"../datasets/wikipedia_book_corpus/data/\" + filename\n","  with open(filepath) as file:\n","    for line in file:\n","      data.append(line.rstrip())\n","\n","wiki_book_vocab = create_vocab_for_most_n_frequent_words(data, vocab_size, stopwords = stops)\n","#print_vocab(wiki_book_vocab)\n","save_vocab_to_file(wiki_book_vocab, \"wiki_book_vocabulary.json\")"],"metadata":{"id":"HxD-22r1hWm7"},"execution_count":null,"outputs":[]}]}